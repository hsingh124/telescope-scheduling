\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{marginnote}
\usepackage{biblatex}
\addbibresource{References.bib}
\usepackage{geometry}
\geometry{
    a4paper,
    total={170mm,257mm},
    top=20mm,
}
\setlength\parindent{0pt}
\setlength{\parskip}{0.5em}

\title{Research notes on Telescope Scheduling}
\author{Harpreet Singh}
\date{2021}

\begin{document}

\maketitle

\newgeometry{
    left=10mm,
    top=20mm,
}

\section*{\citetitle*{naghib2019framework}}

\marginnote{This is a margin note using the geometry package, set at 
3cm vertical offset to the line it is typeseted.}[1cm]

\subsection*{LSST}

The Large Synoptic Survey Telescope (LSST) is a large,
ground-based optical survey that will image half of the sky
every few nights from Cerro Pachon in Northern Chile. LSST
comprises an 8.4 m primary mirror and a 3.2 gigapixel camera.
With a 9.6 \(deg^2\) field of view, it will visit each part of its
18,000 \(deg^2\) primary survey area about 1000 times over the
course of 10 yr. Each visit will likely comprise a 15 s pair of
exposures with a single-visit depth of about 24.5 mag (AB)
(in the six bands u, g, r, i, z, and y). The revolutionary role of
this telescope calls for no less than optimal operation.

There are four primary science drivers for the LSST project:
the characterization of dark energy through the multiple
cosmological probes (e.g., gravitational weak lensing, luminosity distances from Type Ia supernovae, and baryon acoustic
oscillations), mapping the 3D distribution of stars within our
Galaxy, a census of solar system objects within the solar
system, and a detailed study of the transient and variable
universe. Each of these objectives has a different set of
constraints and requirements on how the observations are made
(e.g., the cadence of the observations, the number of filters as a
function of time, the acceptable air-mass range for an
observation). 

\subsection*{Problem Defination and Constraints}


Earlier algorithmic approaches to the scheduling of groundbased telescopes 
are heavily based on observation proposals.
Proposals are handcrafted sequences of scripted astronomical
observations. They are generally tested only for feasibility
(e.g., that a set of fields were visible, or lie within a specified
air-mass range, or within a window in time), but not necessarily
for optimality.

More recently, the development of more expensive groundbased instruments with complex missions made it impossible to
rely solely on handcrafted proposals. The need for more
efficient use of the instrument’s time led to the development of decision-making algorithms to optimize their science output.
The scheduling at the single-visit level is referred to as optimal scheduling and it is
stated that the optimal scheduling requires reevaluating the
future sequence of observations once it is interrupted, but the
necessary extra computation is neither affordable nor fast
enough. However, in this paper we show that the scheduling in
the single-visit level, optimal scheduling, can be quickly
recovered after an interruption, if a memoryless framework is
used. Thus, the optimality does not necessarily need to be
sacrificed because of the limited computational resources.

To run a ground-based telescope with multiple science
objectives, such as LSST, the scheduler has to offer
\textit{controllability}, \textit{adjustability}, and \textit{recoverability}


\textit{MORE TO ADD!!!!!!!!!!!!}

\section*{\citetitle*{bellm2019zwicky}}

\subsection*{Introduction and ZTF}

In this work we consider the specific scheduling problem of
a single-telescope ground-based wide-field imaging survey. We
are focused on its application to the Zwicky Transient Facility
project, which imposes some specific requirements, but our
formalism is relevant for other time-domain surveys, such as
those conducted with the LSST, the Dark Energy Camera, 
and Hyper Suprime-cam. Minor modifications would enable its
use by multi-telescope surveys such as the Asteroid Terrestrialimpact
Last Alert System, PanSTARRS, the All-sky Automated Survey for
Supernovae, and BlackGEM.

Simply stated, the scheduling problem to be solved is to
determine which fields to observe in what order, with a goal of
maximizing an objective function while achieving the desired
temporal spacing of observations (“cadence”). Optimizing the
survey schedule provides a greater quantity of high-quality
data, increasing the scientific output of the survey.

Specific requirements imposed on the ZTF scheduler
included the following:

\begin{enumerate}
    \item Select pointings from a fixed field grid
    \item  Operate in both simulation mode and on-sky using the same scheduling code.
    \item Conduct several surveys, maintaining strict independence of their observations and balancing observing time between programs.
    \item Provide interfaces for conducting TOO observations and monitoring scheduler status.
    \item Recover appropriately from interruptions and weather losses.
    \item Maximize an observing efficiency metric and prioritize cadence control.
\end{enumerate}

\subsection*{Problem}

The ZTF scheduler attempts to maximize the total
number of exposures taken per night, weighted by the spatial
volume probed by each, and subject to the constraints imposed by
program balance and cadence. If the observing
cadences are well chosen, maximizing this quantity will maximize
the transient discovery rate. Bellm explores the relationship
between the chosen observing cadences, a survey’s volumetric
and areal survey rates, and the transient detection rate.

Neglecting cosmological effects, the volume \(V_{lim}\) probed by
a given exposure is proportional to the cube of the limiting
distance \(d_{lim}\) a transient of fiducial absolute magnitude M can
be detected given the limiting magnitude \(m_{lim}: V_{lim} \propto d_{lim}^3\),
where \(d = 10^{0.2(m_{lim}-M+5)}\)pc. The volumetric
weighting per exposure is thus \[V = 10^{0.6(m_{lim} - 21)},\]
where we have absorbed constant factors and normalized to a
convenient limiting magnitude for ZTF.

\subsection*{Constraints}
This weighting combines in a self-consistent way many
factors that are intuitively relevant for assessing whether an
image is “good”: the limiting magnitude depends on the filter,
seeing, airmass, and sky brightness. We use a model
to predict the variation in limiting magnitude and
hence our metric as a function of these time-varying inputs.
Accordingly, our optimization will naturally select exposures
near zenith and away from the moon; but by combining them in
a single scalar the optimization can coherently trade these
factors against one another as they change through the night.

Our metric deliberately does not contain factors that account
for relative scientific priority or cadence. These concerns have
no general quantitative relationship to our objective function or
each other.14 Instead, we use the structure of the optimization
algorithm (Section 4) to impose these constraints.

Our optimization algorithm (Section 4) maximizes the
summed metric over an entire night. In cases where a greedy
algorithm is more convenient, it is simple to define an
instantaneous volumetric survey speed, \[V \propto 10^{0.6m_{lim}}/(t_{exp}+t_{OH})\]
that normalizes the volume probed in an exposure by the time
required to obtain it, a sum of the exposure time \(t_{exp}\) and any
readout or slew overheads \(t_{OH}\).


\subsection*{Future Improvements}

\section*{Extra notes and observations}
\begin{enumerate}
    \item In a lot of papers, it was observed that the observation requests were weighted by a priority assigned to them by a committee or group.
\end{enumerate}

\printbibliography

\end{document}